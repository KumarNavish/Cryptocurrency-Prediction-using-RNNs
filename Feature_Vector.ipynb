{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature_Vector.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KumarNavish/Cryptocurrency-Prediction-using-RNNs/blob/master/Feature_Vector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-08T23:40:59.830569Z",
          "start_time": "2019-02-08T23:40:56.801163Z"
        },
        "id": "6bDqNfB7oKfC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !pip install contractions\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "import re\n",
        "import contractions\n",
        "import unicodedata\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "# import nltk\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-07T18:37:40.634284Z",
          "start_time": "2019-02-07T18:37:39.831388Z"
        },
        "id": "WjdOf_s8oKfK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "######################## LOAD DATASET ######################\n",
        "\n",
        "# The file is being fetched from remote - the following needs to run only once\n",
        "\n",
        "import requests\n",
        "response_dataset = requests.get(url=\"https://raw.githubusercontent.com/hackathoniima/ICADABAI2019/master/dataset.json\")\n",
        "data = response_dataset.json()\n",
        "\n",
        "# Alterativly you can load the file from local as well by the following code.\n",
        "# To do so, comment out the above and uncomment the below.\n",
        "#\n",
        "# with open(\"dataset.json\", \"r\") as fil:\n",
        "#   data = json.load(fil)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-07T18:37:49.341424Z",
          "start_time": "2019-02-07T18:37:49.335490Z"
        },
        "id": "p-h7uTFUoKfM",
        "colab_type": "code",
        "colab": {},
        "outputId": "1c674611-13e0-47c3-b5b0-ad64a4a26813"
      },
      "cell_type": "code",
      "source": [
        "data[:2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Let me explain to you why I believe this : For years, SunTrust advertised and promoted in big letters their XXXX XXXX debit card.',\n",
              " 'With that card, an account holder could earn XXXX mile for every {$1.00} spent ( for debit and credit card transactions ).']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "p5hG-0sQoKfS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "regex_for_XXX = r'[^a-zA-Z0-9]{1}[X][X \\/-]{0,}[X]'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-07T18:40:32.221494Z",
          "start_time": "2019-02-07T18:40:31.879094Z"
        },
        "id": "ctb-wd2ToKfV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################## Check for the pattern XXXX ##########################\n",
        "\n",
        "regex_for_XXX = r'[^a-zA-Z0-9]{1}[X][X \\/-]{0,}[X]'\n",
        "m = []\n",
        "new_data = []\n",
        "find_start_index = []\n",
        "match_XXXX_len = []\n",
        "for row in data: \n",
        "  match = re.findall(regex_for_XXX,row)\n",
        "#   print(match)\n",
        "  if len(match) != 1:\n",
        "    match2 = re.findall(\"[X][X \\/-]{0,}[X]\",row)\n",
        "    if len(match2) != 1:\n",
        "      print(row)\n",
        "      continue\n",
        "    else:\n",
        "      match2 = re.findall(\"[X][X \\/-]{0,}[X]\",match2[0])\n",
        "\n",
        "      if match2[0] not in m:\n",
        "        m.append(match2[0])\n",
        "\n",
        "      new_data.append(row)\n",
        "      find_start_index.append(re.search(match2[0],row).start())\n",
        "      match_XXXX_len.append(len(match2[0]))\n",
        "\n",
        "  else:\n",
        "    match2 = re.findall(\"[X][X \\/-]{0,}[X]\",match[0])\n",
        "    find_start_index.append(re.search(match2[0],row).start())\n",
        "    match_XXXX_len.append(len(match2[0]))\n",
        "\n",
        "    if match2[0] not in m:\n",
        "      m.append(match2[0])\n",
        "\n",
        "    new_data.append(row)\n",
        "\n",
        "data = new_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ha9DK9kJoKfY",
        "colab_type": "raw"
      },
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "Preprocessing: BASIC WORKFLOW\n",
        "\n",
        "#     Split about '-' or '_'\n",
        "    sents = sents.replace('-',' - ').replace('_',' _ ')\n",
        "#     I'm -> I am || gotta -> got to et al\n",
        "    sents = contractions.fix(sents) #, slang = True)\n",
        "#     tokenize\n",
        "#     Number to words\n",
        "    sents = replace_numbers(sents)\n",
        "#     Remove non-ASCII Chars, emoji ...\n",
        "    sents = remove_non_ascii(sents)\n",
        "#     To lowercase ############# Commented out for now\n",
        "    sents = to_towercase(sents)\n",
        "#     Remove punctuation\n",
        "    sents = rm_punc(sents)\n",
        "#     Remove stopwords\n",
        "    sents = rm_stopwords(sents)\n",
        "#     Lemmatize\n",
        "    sents = lemmatize_verbs(sents)\n",
        "#     sents = sents.lower()\n",
        "\"\"\""
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-07T18:41:02.804593Z",
          "start_time": "2019-02-07T18:41:02.800151Z"
        },
        "id": "pBvEF35uoKfZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_non_ascii(words): ###### Remove non-ASCII characters from list of tokenized words\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-07T18:41:07.889021Z",
          "start_time": "2019-02-07T18:41:07.886270Z"
        },
        "id": "kr3vlSZnoKfc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_towercase(words): ###### Convert all characters to lowercase from tokenized list\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-07T18:41:12.639707Z",
          "start_time": "2019-02-07T18:41:12.631507Z"
        },
        "id": "4li1bzo5oKfe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rm_punc(sents): #rm punctuation from a list of words\n",
        "    new_words = []\n",
        "    for word in sents:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-07T18:41:21.357147Z",
          "start_time": "2019-02-07T18:41:21.352518Z"
        },
        "id": "RrWz7GNmoKfh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rm_stopwords(sents):\n",
        "    stopwords_list = nltk.corpus.stopwords.words('english')\n",
        "    new_words = []\n",
        "    for word in sents:\n",
        "        if word not in stopwords_list:\n",
        "            new_words.append(word)\n",
        "    return(new_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-07T18:41:27.714044Z",
          "start_time": "2019-02-07T18:41:27.710582Z"
        },
        "id": "UF0N5_sKoKfl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-07T18:41:33.440604Z",
          "start_time": "2019-02-07T18:41:33.432016Z"
        },
        "id": "-SC179CZoKfo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_string(sents):\n",
        "  \n",
        "  \n",
        "  # Split about '-' or '_'\n",
        "  sents = sents.replace('-',' - ').replace('_',' _ ')\n",
        "\n",
        "  # I'm -> I am || gotta -> got to \n",
        "  sents = contractions.fix(sents) #, slang = True)\n",
        "\n",
        "  # Tokenize\n",
        "  sents = nltk.word_tokenize(sents)\n",
        "    \n",
        "  # Remove non-ASCII Chars, emoji ...\n",
        "  sents = remove_non_ascii(sents)\n",
        "\n",
        "  # To lowercase\n",
        "#   sents = to_towercase(sents)\n",
        "\n",
        "  # Remove punctuation\n",
        "  sents = rm_punc(sents)\n",
        "  \n",
        "  # Remove stopwords\n",
        "#   sents = rm_stopwords(sents)\n",
        "\n",
        "  # Lemmatize\n",
        "  sents = lemmatize_verbs(sents)\n",
        "  \n",
        "  return sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "swSKMeoPoKfq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "############################## End of preprocessing ###########################\n",
        "\n",
        "########## TO NOTE #########\n",
        "* Stopwords\n",
        "* Lowercase"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-07T21:25:28.575388Z",
          "start_time": "2019-02-07T21:24:57.007278Z"
        },
        "id": "XPUKZYO7oKfs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "############## Divide each string of training data about the matched part for pattern -XXXX ##########\n",
        "\n",
        "# formatted_data = []\n",
        "\n",
        "# for i in range(len(data)):\n",
        "#   row = data[i]\n",
        "#   this_entity = {}\n",
        "#   this_entity[\"left_part\"] = preprocess_string((row[ : find_start_index[i]]).strip())\n",
        "#   this_entity[\"matched_part\"] = (row[find_start_index[i] : find_start_index[i] +\n",
        "#                                                      match_XXXX_len [i]]).strip()\n",
        "#   this_entity[\"right_part\"] = preprocess_string((row[find_start_index[i] +\n",
        "#                                         match_XXXX_len [i] : ]).strip())\n",
        "#   formatted_data.append(this_entity)\n",
        "\n",
        "# with open('formatted_data.pkl','wb') as fp:\n",
        "#     pickle.dump(formatted_data,fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T4cwoDxBoKfu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loading the Saved data | Run  from hereon after"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-08T23:41:07.202759Z",
          "start_time": "2019-02-08T23:41:06.707266Z"
        },
        "id": "rIKvGzzaoKfv",
        "colab_type": "code",
        "colab": {},
        "outputId": "db4e1ad9-bb9d-46fd-b989-30abaec89bfb"
      },
      "cell_type": "code",
      "source": [
        "with open('formatted_data.pkl','rb') as fp:\n",
        "    formatted_data=pickle.load(fp)\n",
        "formatted_data[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'left_part': ['Let',\n",
              "  'me',\n",
              "  'explain',\n",
              "  'to',\n",
              "  'you',\n",
              "  'why',\n",
              "  'I',\n",
              "  'believe',\n",
              "  'this',\n",
              "  'For',\n",
              "  'years',\n",
              "  'SunTrust',\n",
              "  'advertise',\n",
              "  'and',\n",
              "  'promote',\n",
              "  'in',\n",
              "  'big',\n",
              "  'letter',\n",
              "  'their'],\n",
              " 'matched_part': 'XXXX XXXX',\n",
              " 'right_part': ['debit', 'card']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "E_llIvUzoKfz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "* !unzip glove*.zip"
      ]
    },
    {
      "metadata": {
        "id": "OrdnoTo3oKf0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Word2Vec_embedding Dict:"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-08T23:41:11.877442Z",
          "start_time": "2019-02-08T23:41:09.672798Z"
        },
        "id": "CjmQGudvoKf1",
        "colab_type": "code",
        "colab": {},
        "outputId": "9b6a4ea3-ea03-4b94-aa21-442ce8471ddc"
      },
      "cell_type": "code",
      "source": [
        "# print('Indexing word vectors.')\n",
        "\n",
        "# embeddings_index = {}\n",
        "# with open('glove.6B.50d.txt', encoding='utf-8') as fp:\n",
        "#     for line in fp:\n",
        "#         values = line.split()\n",
        "#         word = values[0]\n",
        "#         coefs = np.asarray(values[1:],dtype=np.float32)\n",
        "#         embeddings_index[word] = coefs\n",
        "\n",
        "# with open('embeddings_index.pkl','wb') as fp:\n",
        "#     pickle.dump(embeddings_index,fp)\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "with open('embeddings_index.pkl','rb') as fp:\n",
        "    embeddings_index=pickle.load(fp)\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8Ij8VFf1oKf5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Applying padding in sequence lengths (required for dynamic_rnn_input)"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-08T23:53:19.119028Z",
          "start_time": "2019-02-08T23:51:48.252278Z"
        },
        "scrolled": true,
        "id": "EywnKNj8oKf6",
        "colab_type": "code",
        "colab": {},
        "outputId": "b0bc831c-4b72-4540-df1d-75e028064cfa"
      },
      "cell_type": "code",
      "source": [
        "#Finding Max sequence length in both left & right context vector\n",
        "l,r = zip(*[(len(formatted_data[i]['left_part']),len(formatted_data[i]['right_part'])) for i in range(len(formatted_data))])\n",
        "max_l,max_r = max(l),max(r)\n",
        "\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "vector_len=50\n",
        "# Converting Word to Vectors (if words are present in pretrained model):\n",
        "def create_word2vec(data):\n",
        "#     context_vec=[]\n",
        "    with open('context_vec.pkl','wb') as fp:\n",
        "        for index,i in tqdm(enumerate(data)):\n",
        "            left=right=[]\n",
        "\n",
        "            # For Left Part\n",
        "            left = [list(embeddings_index[j]) for j in i['left_part'] if j in embeddings_index.keys()]\n",
        "            # Padding | making same length sequences\n",
        "            if len(left)<max_l:\n",
        "                left += np.zeros((max_l-len(left),vector_len),dtype=np.float32).tolist()\n",
        "\n",
        "\n",
        "            # For right part   \n",
        "            right = [list(embeddings_index[j]) for j in i['right_part'] if j in embeddings_index.keys()]\n",
        "            # Padding | making same length sequences\n",
        "            if len(right)<max_r:\n",
        "                right += np.zeros((max_r-len(right),vector_len),dtype=np.float32).tolist() \n",
        "                \n",
        "            pickle.dump({'left_ContextVec':left,'right_ContextVec':right},fp)            \n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "        \n",
        "create_word2vec(formatted_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30916it [01:28, 348.80it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-08T23:53:19.140129Z",
          "start_time": "2019-02-08T23:53:19.122778Z"
        },
        "id": "RQY_MRDZoKf_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "context_vec = []# is huge in memory ~ 25.6gb\n",
        "i=0\n",
        "with open('context_vec.pkl', 'rb') as fr:\n",
        "    try:\n",
        "        while i<4:\n",
        "            context_vec.append(pickle.load(fr))\n",
        "            i+=1\n",
        "    except EOFError:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-08T23:53:20.024782Z",
          "start_time": "2019-02-08T23:53:19.143987Z"
        },
        "id": "NCTqQ4PgoKgB",
        "colab_type": "code",
        "colab": {},
        "outputId": "59ca0fc2-9771-4bb6-9c41-adc6dd4f9f2c"
      },
      "cell_type": "code",
      "source": [
        "len(context_vec)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "NmQVgzR9oKgF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## After Preprocessing and word2vec generation- Further Process"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-08T23:53:20.119256Z",
          "start_time": "2019-02-08T23:53:20.028697Z"
        },
        "id": "W0LxtnS9oKgG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-07T22:11:54.471238Z",
          "start_time": "2019-02-07T22:11:54.444679Z"
        },
        "id": "rTwYxPDloKgJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* This is the recommended initializer for NN weights:\n",
        "* tf.truncated_normal_initializer()"
      ]
    },
    {
      "metadata": {
        "id": "V9um8sA-oKgK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Left and Right Context Vectors(*not complete):"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-09T00:25:42.891556Z",
          "start_time": "2019-02-09T00:25:42.881392Z"
        },
        "id": "xknGhHNwoKgL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "left_cv = np.array([np.array(context_vec[i]['left_ContextVec'],dtype=np.float32) for i in range(4)])\n",
        "right_cv = np.array([np.array(context_vec[i]['right_ContextVec'],dtype=np.float32) for i in range(4)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A6_kxy89oKgQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bidirectional LSTM, Dynamic RNN"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-09T00:04:48.280940Z",
          "start_time": "2019-02-09T00:04:47.975003Z"
        },
        "id": "UCSLn6uIoKgQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rnn_size, keep_prob = 64,0.85\n",
        "def bidirectional_lstm(input_data, rnn_size, keep_prob):\n",
        "#     with open('context_vec.pkl', 'rb') as fr:\n",
        "#         try:\n",
        "#             while True:\n",
        "#                 output.append(input_data)\n",
        "#         except EOFError:\n",
        "#             pass\n",
        "            \n",
        "    with tf.variable_scope('Hello',reuse=tf.AUTO_REUSE):\n",
        "\n",
        "        # forward input embedding\n",
        "        cell_fw = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.truncated_normal_initializer(-0.1, 0.1, seed=2))\n",
        "        cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob = keep_prob)\n",
        "\n",
        "        # backward input embedding\n",
        "        cell_bw = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.truncated_normal_initializer(-0.1, 0.1, seed=2))\n",
        "        cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob = keep_prob)\n",
        "\n",
        "        # Outputs is a tuple of backward and forward input embedding\n",
        "        outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
        "                                                          cell_bw, \n",
        "                                                          input_data,\n",
        "                                                          dtype=tf.float32)\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kSo6-Z1joKgT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Building the Session"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-09T00:55:00.783167Z",
          "start_time": "2019-02-09T00:53:53.021711Z"
        },
        "id": "p0VNDggDoKgT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inputs = tf.placeholder(dtype=tf.float32, shape=left_cv.shape)\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    left_hidden_fw,left_hidden_bw = sess.run(bidirectional_lstm(inputs, rnn_size, keep_prob), \n",
        "                                        {inputs:left_cv})\n",
        "    \n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    \n",
        "inputs = tf.placeholder(dtype=tf.float32, shape=right_cv.shape)\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    right_hidden_fw,right_hidden_bw = sess.run(bidirectional_lstm(inputs, rnn_size, keep_prob), \n",
        "                                        {inputs:right_cv})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-09T00:55:00.788786Z",
          "start_time": "2019-02-09T00:55:00.784739Z"
        },
        "id": "R2q0YwTqoKgW",
        "colab_type": "code",
        "colab": {},
        "outputId": "da47c2ea-ca62-4a18-9ab1-02a9efeafd28"
      },
      "cell_type": "code",
      "source": [
        "left_hidden_fw[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.40927257e-02, -9.69089940e-02,  6.76873028e-02, ...,\n",
              "        -5.02436385e-02, -4.39806134e-02,  1.00969449e-01],\n",
              "       [ 7.03788316e-03, -1.11870885e-01, -8.80033821e-02, ...,\n",
              "        -1.58782095e-01, -6.47772253e-02, -3.22080031e-02],\n",
              "       [ 7.72010088e-02,  5.02228737e-03, -4.52372245e-02, ...,\n",
              "        -1.75192028e-01,  9.27934796e-02, -1.32149622e-01],\n",
              "       ...,\n",
              "       [-9.20629617e-09,  1.43528177e-06, -2.07196854e-06, ...,\n",
              "        -3.51621935e-07, -1.63170353e-06,  1.21182529e-06],\n",
              "       [-1.23268808e-08,  1.30279216e-06, -1.87978276e-06, ...,\n",
              "        -3.04943285e-07, -1.42877650e-06,  1.16133344e-06],\n",
              "       [-5.99632699e-08,  1.14378372e-06, -1.73965395e-06, ...,\n",
              "        -3.00651777e-07, -1.28322768e-06,  1.07255380e-06]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "metadata": {
        "id": "3smC-ztIoKga",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pairwise ordering of fw and bw hidden vectors"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-09T00:55:05.746463Z",
          "start_time": "2019-02-09T00:55:05.733522Z"
        },
        "id": "XHyL1RlroKgb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = left_hidden_fw.shape[0]\n",
        "def input_format(batch, fw, bw):\n",
        "    merged = []\n",
        "    for i in range(left_hidden_fw.shape[0]):\n",
        "        temp = np.zeros(((2*fw.shape[1]),fw.shape[2]),dtype=np.float32)\n",
        "        for j in range(fw.shape[1]):\n",
        "            temp[2*j]= fw[i][j]\n",
        "            temp[2*j+1] = bw[i][j]\n",
        "        merged.append(temp)\n",
        "    return np.array(merged)\n",
        "left_hidden_merged = input_format(batch, left_hidden_fw, left_hidden_bw)\n",
        "right_hidden_merged = input_format(batch, right_hidden_fw, right_hidden_bw )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-09T00:55:14.693584Z",
          "start_time": "2019-02-09T00:55:14.685334Z"
        },
        "id": "xpzs1G_PoKge",
        "colab_type": "code",
        "colab": {},
        "outputId": "ebd5cf09-c87c-4f34-9349-66ada206bc72"
      },
      "cell_type": "code",
      "source": [
        "right_hidden_merged[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-7.56302699e-02, -7.51577690e-02, -1.42197892e-01, ...,\n",
              "        -2.02055603e-01,  1.58784941e-01,  1.28846183e-01],\n",
              "       [-1.76053941e-01, -8.22217204e-03,  2.00145263e-02, ...,\n",
              "        -3.24034572e-01,  2.26524055e-01, -6.02251925e-02],\n",
              "       [-1.41308442e-01,  6.39979020e-02, -8.28666165e-02, ...,\n",
              "        -2.94512570e-01,  3.33593398e-01,  7.43647665e-02],\n",
              "       ...,\n",
              "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
              "       [-1.40627865e-08,  6.08729920e-08, -5.29557376e-08, ...,\n",
              "        -3.29368688e-09, -5.54569617e-08,  4.76004587e-08],\n",
              "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "metadata": {
        "id": "uG0QeYihoSYS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## This Code Block is under construction"
      ]
    },
    {
      "metadata": {
        "id": "5epFQgQUoKgh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dim_hidden_state,num_hidden_layer1 = 100, 64\n",
        "weights = {'e_left': tf.Variable(tf.random_normal([dim_hidden_state,num_hidden_layer1])),\n",
        "           'e_right': tf.Variable(tf.random_normal([dim_hidden_state,num_hidden_layer1])),\n",
        "           'a_left': tf.Variable(tf.random_normal([num_hidden_layer1,1])),\n",
        "           'a_right': tf.Variable(tf.random_normal([num_hidden_layer1,1]))}\n",
        "\n",
        "# NN with 1 hidden Layer:\n",
        "e_l = tf.nn.tanh(tf.matmul(new_outputs,weights['e_left']))\n",
        "e_r = tf.nn.tanh(tf.matmul(new_outputs,weights['e_right']))\n",
        "\n",
        "# Multiplying the above vectors(dim=(30916,num_hidden_layer1)) with W_a's to get a scalar\n",
        "e_l = tf.nn.exp(tf.matmul(e_l,weights['a_left']))\n",
        "e_r = tf.nn.exp(tf.matmul(e_r,weights['a_right']))\n",
        "\n",
        "# Normalizing above scalars:\n",
        "a_l = e_l/(e_l + e_r)\n",
        "a_r = e_r/(e_l + e_r)\n",
        "\n",
        "# Final context vector:\n",
        "V_c = []\n",
        "for i,j in zip(a_l,a_r):\n",
        "    V_c.append(tf.add(tf.scalar_mul(i,left_context_vec),tf.scalar_mul(j,right_context_vec)))\n",
        "V_c = tf.array(V_c)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jTLJhsiKoKgk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}